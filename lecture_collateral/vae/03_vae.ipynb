{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Example\n",
    "\n",
    "From https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DL4DS/sp2025_homeworks/blob/main/lecture_collateral/vae/03_vae.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple VAE Trained on Random Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Define the encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Define the latent representation\n",
    "        self.fc_mu = nn.Linear(16, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(16, latent_dim)\n",
    "\n",
    "        # Define the decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std*eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the VAE on the normal data\n",
    "vae = VAE(input_dim=30, latent_dim=10)\n",
    "\n",
    "# Generate random input data to test the model\n",
    "data = torch.randn(100, 30)\n",
    "optimizer = torch.optim.Adam(vae.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = VAE(input_dim=30, latent_dim=10)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Generate random input data to test the model\n",
    "# We run it through a sigmoid to ensure it is between 0 and 1\n",
    "data = torch.sigmoid(torch.randn(100, 30))\n",
    "\n",
    "# Define our reconstruction loss function\n",
    "loss_fn = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: reconstruction_loss = 0.6982, kl_loss = 11.1982, total_loss = 11.8963\n",
      "Epoch 1: reconstruction_loss = 0.6976, kl_loss = 10.3272, total_loss = 11.0248\n",
      "Epoch 2: reconstruction_loss = 0.6970, kl_loss = 9.5823, total_loss = 10.2793\n",
      "Epoch 3: reconstruction_loss = 0.6966, kl_loss = 8.9458, total_loss = 9.6423\n",
      "Epoch 4: reconstruction_loss = 0.6967, kl_loss = 8.4035, total_loss = 9.1002\n",
      "Epoch 5: reconstruction_loss = 0.6961, kl_loss = 7.9388, total_loss = 8.6349\n",
      "Epoch 6: reconstruction_loss = 0.6960, kl_loss = 7.5405, total_loss = 8.2365\n",
      "Epoch 7: reconstruction_loss = 0.6951, kl_loss = 7.1933, total_loss = 7.8884\n",
      "Epoch 8: reconstruction_loss = 0.6958, kl_loss = 6.8829, total_loss = 7.5787\n",
      "Epoch 9: reconstruction_loss = 0.6957, kl_loss = 6.5900, total_loss = 7.2856\n",
      "Epoch 10: reconstruction_loss = 0.6955, kl_loss = 6.3087, total_loss = 7.0042\n",
      "Epoch 11: reconstruction_loss = 0.6951, kl_loss = 6.0331, total_loss = 6.7282\n",
      "Epoch 12: reconstruction_loss = 0.6954, kl_loss = 5.7469, total_loss = 6.4423\n",
      "Epoch 13: reconstruction_loss = 0.6948, kl_loss = 5.4539, total_loss = 6.1487\n",
      "Epoch 14: reconstruction_loss = 0.6948, kl_loss = 5.1683, total_loss = 5.8631\n",
      "Epoch 15: reconstruction_loss = 0.6940, kl_loss = 4.8977, total_loss = 5.5917\n",
      "Epoch 16: reconstruction_loss = 0.6948, kl_loss = 4.6437, total_loss = 5.3385\n",
      "Epoch 17: reconstruction_loss = 0.6945, kl_loss = 4.4108, total_loss = 5.1053\n",
      "Epoch 18: reconstruction_loss = 0.6940, kl_loss = 4.1983, total_loss = 4.8923\n",
      "Epoch 19: reconstruction_loss = 0.6943, kl_loss = 4.0040, total_loss = 4.6984\n",
      "Epoch 20: reconstruction_loss = 0.6944, kl_loss = 3.8257, total_loss = 4.5201\n",
      "Epoch 21: reconstruction_loss = 0.6940, kl_loss = 3.6613, total_loss = 4.3552\n",
      "Epoch 22: reconstruction_loss = 0.6941, kl_loss = 3.5086, total_loss = 4.2028\n",
      "Epoch 23: reconstruction_loss = 0.6936, kl_loss = 3.3655, total_loss = 4.0591\n",
      "Epoch 24: reconstruction_loss = 0.6936, kl_loss = 3.2296, total_loss = 3.9232\n",
      "Epoch 25: reconstruction_loss = 0.6942, kl_loss = 3.0994, total_loss = 3.7936\n",
      "Epoch 26: reconstruction_loss = 0.6939, kl_loss = 2.9730, total_loss = 3.6669\n",
      "Epoch 27: reconstruction_loss = 0.6935, kl_loss = 2.8491, total_loss = 3.5426\n",
      "Epoch 28: reconstruction_loss = 0.6933, kl_loss = 2.7266, total_loss = 3.4199\n",
      "Epoch 29: reconstruction_loss = 0.6936, kl_loss = 2.6049, total_loss = 3.2985\n",
      "Epoch 30: reconstruction_loss = 0.6939, kl_loss = 2.4840, total_loss = 3.1780\n",
      "Epoch 31: reconstruction_loss = 0.6936, kl_loss = 2.3647, total_loss = 3.0582\n",
      "Epoch 32: reconstruction_loss = 0.6937, kl_loss = 2.2477, total_loss = 2.9413\n",
      "Epoch 33: reconstruction_loss = 0.6927, kl_loss = 2.1342, total_loss = 2.8270\n",
      "Epoch 34: reconstruction_loss = 0.6935, kl_loss = 2.0255, total_loss = 2.7190\n",
      "Epoch 35: reconstruction_loss = 0.6932, kl_loss = 1.9225, total_loss = 2.6156\n",
      "Epoch 36: reconstruction_loss = 0.6931, kl_loss = 1.8259, total_loss = 2.5190\n",
      "Epoch 37: reconstruction_loss = 0.6930, kl_loss = 1.7358, total_loss = 2.4288\n",
      "Epoch 38: reconstruction_loss = 0.6932, kl_loss = 1.6521, total_loss = 2.3453\n",
      "Epoch 39: reconstruction_loss = 0.6930, kl_loss = 1.5741, total_loss = 2.2671\n",
      "Epoch 40: reconstruction_loss = 0.6933, kl_loss = 1.5006, total_loss = 2.1939\n",
      "Epoch 41: reconstruction_loss = 0.6932, kl_loss = 1.4308, total_loss = 2.1239\n",
      "Epoch 42: reconstruction_loss = 0.6928, kl_loss = 1.3634, total_loss = 2.0562\n",
      "Epoch 43: reconstruction_loss = 0.6932, kl_loss = 1.2977, total_loss = 1.9909\n",
      "Epoch 44: reconstruction_loss = 0.6934, kl_loss = 1.2334, total_loss = 1.9268\n",
      "Epoch 45: reconstruction_loss = 0.6926, kl_loss = 1.1703, total_loss = 1.8630\n",
      "Epoch 46: reconstruction_loss = 0.6929, kl_loss = 1.1087, total_loss = 1.8016\n",
      "Epoch 47: reconstruction_loss = 0.6932, kl_loss = 1.0490, total_loss = 1.7422\n",
      "Epoch 48: reconstruction_loss = 0.6932, kl_loss = 0.9917, total_loss = 1.6849\n",
      "Epoch 49: reconstruction_loss = 0.6931, kl_loss = 0.9372, total_loss = 1.6304\n",
      "Epoch 50: reconstruction_loss = 0.6931, kl_loss = 0.8859, total_loss = 1.5790\n",
      "Epoch 51: reconstruction_loss = 0.6930, kl_loss = 0.8377, total_loss = 1.5307\n",
      "Epoch 52: reconstruction_loss = 0.6929, kl_loss = 0.7927, total_loss = 1.4856\n",
      "Epoch 53: reconstruction_loss = 0.6928, kl_loss = 0.7505, total_loss = 1.4433\n",
      "Epoch 54: reconstruction_loss = 0.6925, kl_loss = 0.7108, total_loss = 1.4034\n",
      "Epoch 55: reconstruction_loss = 0.6931, kl_loss = 0.6733, total_loss = 1.3664\n",
      "Epoch 56: reconstruction_loss = 0.6927, kl_loss = 0.6376, total_loss = 1.3303\n",
      "Epoch 57: reconstruction_loss = 0.6922, kl_loss = 0.6035, total_loss = 1.2956\n",
      "Epoch 58: reconstruction_loss = 0.6930, kl_loss = 0.5708, total_loss = 1.2638\n",
      "Epoch 59: reconstruction_loss = 0.6927, kl_loss = 0.5396, total_loss = 1.2322\n",
      "Epoch 60: reconstruction_loss = 0.6925, kl_loss = 0.5098, total_loss = 1.2024\n",
      "Epoch 61: reconstruction_loss = 0.6926, kl_loss = 0.4816, total_loss = 1.1742\n",
      "Epoch 62: reconstruction_loss = 0.6925, kl_loss = 0.4549, total_loss = 1.1474\n",
      "Epoch 63: reconstruction_loss = 0.6930, kl_loss = 0.4298, total_loss = 1.1228\n",
      "Epoch 64: reconstruction_loss = 0.6928, kl_loss = 0.4064, total_loss = 1.0992\n",
      "Epoch 65: reconstruction_loss = 0.6926, kl_loss = 0.3844, total_loss = 1.0771\n",
      "Epoch 66: reconstruction_loss = 0.6930, kl_loss = 0.3639, total_loss = 1.0569\n",
      "Epoch 67: reconstruction_loss = 0.6930, kl_loss = 0.3448, total_loss = 1.0377\n",
      "Epoch 68: reconstruction_loss = 0.6925, kl_loss = 0.3269, total_loss = 1.0193\n",
      "Epoch 69: reconstruction_loss = 0.6925, kl_loss = 0.3101, total_loss = 1.0026\n",
      "Epoch 70: reconstruction_loss = 0.6926, kl_loss = 0.2944, total_loss = 0.9869\n",
      "Epoch 71: reconstruction_loss = 0.6928, kl_loss = 0.2796, total_loss = 0.9723\n",
      "Epoch 72: reconstruction_loss = 0.6923, kl_loss = 0.2657, total_loss = 0.9580\n",
      "Epoch 73: reconstruction_loss = 0.6928, kl_loss = 0.2526, total_loss = 0.9454\n",
      "Epoch 74: reconstruction_loss = 0.6925, kl_loss = 0.2404, total_loss = 0.9329\n",
      "Epoch 75: reconstruction_loss = 0.6928, kl_loss = 0.2288, total_loss = 0.9217\n",
      "Epoch 76: reconstruction_loss = 0.6927, kl_loss = 0.2181, total_loss = 0.9107\n",
      "Epoch 77: reconstruction_loss = 0.6927, kl_loss = 0.2080, total_loss = 0.9007\n",
      "Epoch 78: reconstruction_loss = 0.6929, kl_loss = 0.1986, total_loss = 0.8915\n",
      "Epoch 79: reconstruction_loss = 0.6929, kl_loss = 0.1898, total_loss = 0.8827\n",
      "Epoch 80: reconstruction_loss = 0.6925, kl_loss = 0.1817, total_loss = 0.8742\n",
      "Epoch 81: reconstruction_loss = 0.6929, kl_loss = 0.1741, total_loss = 0.8670\n",
      "Epoch 82: reconstruction_loss = 0.6924, kl_loss = 0.1671, total_loss = 0.8595\n",
      "Epoch 83: reconstruction_loss = 0.6925, kl_loss = 0.1606, total_loss = 0.8531\n",
      "Epoch 84: reconstruction_loss = 0.6924, kl_loss = 0.1545, total_loss = 0.8469\n",
      "Epoch 85: reconstruction_loss = 0.6926, kl_loss = 0.1487, total_loss = 0.8413\n",
      "Epoch 86: reconstruction_loss = 0.6926, kl_loss = 0.1434, total_loss = 0.8359\n",
      "Epoch 87: reconstruction_loss = 0.6923, kl_loss = 0.1384, total_loss = 0.8306\n",
      "Epoch 88: reconstruction_loss = 0.6923, kl_loss = 0.1336, total_loss = 0.8260\n",
      "Epoch 89: reconstruction_loss = 0.6929, kl_loss = 0.1292, total_loss = 0.8221\n",
      "Epoch 90: reconstruction_loss = 0.6925, kl_loss = 0.1251, total_loss = 0.8176\n",
      "Epoch 91: reconstruction_loss = 0.6924, kl_loss = 0.1213, total_loss = 0.8137\n",
      "Epoch 92: reconstruction_loss = 0.6929, kl_loss = 0.1178, total_loss = 0.8107\n",
      "Epoch 93: reconstruction_loss = 0.6922, kl_loss = 0.1145, total_loss = 0.8066\n",
      "Epoch 94: reconstruction_loss = 0.6923, kl_loss = 0.1114, total_loss = 0.8037\n",
      "Epoch 95: reconstruction_loss = 0.6924, kl_loss = 0.1085, total_loss = 0.8009\n",
      "Epoch 96: reconstruction_loss = 0.6931, kl_loss = 0.1058, total_loss = 0.7989\n",
      "Epoch 97: reconstruction_loss = 0.6927, kl_loss = 0.1033, total_loss = 0.7960\n",
      "Epoch 98: reconstruction_loss = 0.6925, kl_loss = 0.1010, total_loss = 0.7935\n",
      "Epoch 99: reconstruction_loss = 0.6925, kl_loss = 0.0987, total_loss = 0.7912\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "\n",
    "    # Compute the reconstruction loss\n",
    "    reconstructed, mu, logvar = model(data)\n",
    "    reconstruction_loss = loss_fn(reconstructed, data)\n",
    "\n",
    "    # Compute the KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Compute the total loss\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "    # Backpropagate the gradients and update the model weights\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss values\n",
    "    print(f\"Epoch {epoch}: reconstruction_loss = {reconstruction_loss:.4f}, kl_loss = {kl_loss:.4f}, total_loss = {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

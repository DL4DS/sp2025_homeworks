{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47aea556",
   "metadata": {},
   "source": [
    "## LoRA -- Low-Rank Adaptation\n",
    "\n",
    "Ref: https://iaee.substack.com/p/lora-intuitively-and-exhaustively-explained-e944a6bff46b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb826c1",
   "metadata": {},
   "source": [
    "### Fine-tuning?\n",
    "\n",
    "- process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance\n",
    "- LoRA is a method of fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da08de",
   "metadata": {},
   "source": [
    "### What, and Why, is Fine Tuning?\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1131461-209a-4d88-a297-cf840ccbe2dd_800x166.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b939d",
   "metadata": {},
   "source": [
    "- As the state of the art of machine learning has evolved, expectations of model performance have increased; requiring more complex machine learning approaches to match the demand for heightened performance. In the earlier days of machine learning it was feasible to build a model and train it in a single pass.\n",
    "\n",
    "- This is still a popular strategy for simple problems, but for more complex problems it can be useful to think of training as two parts; “pre-training” then “fine tuning”. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef57dd6",
   "metadata": {},
   "source": [
    "- The general idea is to do an initial training pass on a bulk dataset and to then refine the model on a tailored dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e80b18",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F865d7d46-c49e-4872-9b91-f378d611da71_800x337.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549d73d",
   "metadata": {},
   "source": [
    "- This “pre-training” then “fine tuning” strategy can allow practioners to leverage multiple forms of data and use large pre-trained models for specific tasks. As a result, pre-training then fine tuning is a common and incredibly powerful paradigm. It comes with a few difficulties, though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f1310",
   "metadata": {},
   "source": [
    "### Difficulties with Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12182e",
   "metadata": {},
   "source": [
    "- The most basic form of fine tuning is to use the same exact process you used to pre-train a model to then fine tune that model on new data. You might train a model on a huge corpus of general text data, for instance, then fine tune that model using the same training strategy on a more specific dataset.\n",
    "\n",
    "- this can be very expensive, and it can be very slow.\n",
    "\n",
    "- you would need enough memory to store not only the entire model, but also gradients for every parameter in the entire model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb7033",
   "metadata": {},
   "source": [
    "- LoRA can help us deal with these issues and more, Less GPU memory, less time, and the ability to train larger models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631ec5a",
   "metadata": {},
   "source": [
    "### LoRA in a Nutshell\n",
    "\n",
    "- “Low-Rank Adaptation” (LoRA) is a form of “parameter efficient fine tuning” (PEFT), which allows one to fine tune a large model using a small number of learnable parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf931296",
   "metadata": {},
   "source": [
    "- LoRA employs a few concepts which, when used together, massively improve fine tuning:\n",
    "    - We can think of fine tuning as learning changes to parameters, instead of adjusting parameters themselves.\n",
    "    - We can try to compress those changes into a smaller representation by removing duplicate information.\n",
    "    - We can “load” our changes by simply adding them to the pre-trained parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91588dc",
   "metadata": {},
   "source": [
    "### 1. Fine-tuning as Parameter Changes\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd795783a-75dd-4bb8-b419-699fd9f968ff_800x216.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798362cf",
   "metadata": {},
   "source": [
    "The most basic approach to fine tuning consists of iteratively updating parameters. Just like normal model training, you have the model make an inference, then update the parameters of the model based on how wrong that inference was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b5015",
   "metadata": {},
   "source": [
    "- LoRA thinks of this slightly differently\n",
    "    - Instead of thinking of fine tuning as learning better parameters, you can think of fine tuning as learning parameter changes.\n",
    "    - You can freeze the model parameters, exactly how they are, and learn the changes to those parameters necessary to make the model perform better at the fine tuned task.\n",
    "\n",
    "- This is done very similarly to training, but:\n",
    "    - In LoRA, we freeze the model parameters, and learn the changes to those parameters.\n",
    "    - you have the model make an inference, then update based on how wrong the inference was\n",
    "    - However, instead of updating the model parameters, you update the change in the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f264d16",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F911fd8a7-fc78-4ba8-87e4-e63dfd281c4d_800x359.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d577a",
   "metadata": {},
   "source": [
    "The whole point of LoRA is that we want to make fine tuning smaller and faster, how does adding more data and extra steps allow us to do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec84ae6",
   "metadata": {},
   "source": [
    "### 2. Parameter Change Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa9556",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01892d10-8ef7-476f-86a7-71c92f139207_800x235.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57dfa94",
   "metadata": {},
   "source": [
    "Weights in a network are a matrix -- a matrix has certain properties which can be used to condense information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10060d",
   "metadata": {},
   "source": [
    "#### Matrix Property 1. Linear Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb430b",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F210d267a-0a42-4596-bedf-31761daf7a87_800x327.png)\n",
    "\n",
    "Each of these vectors point in different directions. You can’t squash and stretch one vector to be equal to the other vector. -- Linearly Independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc062a30",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9ea4851-7d50-496e-9e87-a6b564ce4cce_800x370.png)\n",
    "\n",
    "\n",
    "Vectors A and B are pointing in the same exact direction, while vector C is pointing in a different direction. As a result, no matter how you squash and stretch either A or B, they can never be used to describe C. Therefore, C is linearly independent from A and B. However, you can stretch A to equal B , and vice versa, so A and B are linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e5cc8",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca8a54ff-9e4a-41d9-a411-4935acb6ba09_800x386.png)\n",
    "\n",
    "Let's say A and B pointed in slightly different directions.\n",
    "\n",
    "Now A and B can be used together (With some squashing and stretching) to describe C , and likewise A and B can be described by the other vectors. In this situation we would say none of the vectors are linearly independent, because all vectors can be described with other vectors in the matrix.\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c722798-7e18-4210-8d8f-c7b3faf97d9a_800x416.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514c0bf",
   "metadata": {},
   "source": [
    "- Conceptually speaking, linearly independent vectors can be thought of as containing different information, while linearly dependent vectors contain some duplicate information between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcceb92",
   "metadata": {},
   "source": [
    "#### Matrix Property 2. Rank\n",
    "\n",
    "- The idea of rank is to quantify the amount of linear independence within a matrix.\n",
    "- We can break a matrix down into some number of linearly independent vectors; This form of the matrix is called “reduced row echelon form”.\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff66c7ce0-a145-4451-9e85-bad14aa415c9_800x194.png)\n",
    "\n",
    "- By breaking the matrix down into this for --> you can count how many linearly independent vectors can be used to describe the original matrix.\n",
    "-  The number of linearly independent vectors is the “rank” of the matrix. (4 in the above case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb1616",
   "metadata": {},
   "source": [
    "#### Matrix Property 3. Matrix Factors\n",
    "\n",
    "- So, matrices can contain some level of “duplicate information” in the form of linear dependence.\n",
    "-  We can exploit this idea using factorization to represent a large matrix in terms of two smaller matrices. Similarly to how a large number can be represented as the multiplication of two smaller numbers, a matrix can be thought of as the multiplication of two smaller matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5217ac",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99835497-f13d-4e9a-85d4-0540477ffdc1_800x194.png)\n",
    "\n",
    "The two vectors on the left when multiplied together gives the matrix on the right. But crucially, even though they have the same value, the vectors on the left occupy 40% of the size that the matrix on the right occupies.\n",
    "\n",
    "-- The larger the matrix, the more you can save by using this factorization trick."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a474af5",
   "metadata": {},
   "source": [
    "* This idea of factorization is what allows LoRA to occupy such a small memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca852b54",
   "metadata": {},
   "source": [
    "#### The Core Idea Behind LoRA\n",
    "\n",
    "LoRA thinks of tuning not as adjusting parameters, but as learning parameter changes. With LoRA we don’t learn the parameter changes directly, however; we learn the factors of the parameter change matrix.\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F939c41f0-cf60-4cf3-b36d-be60e7aef880_800x222.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3d7d4",
   "metadata": {},
   "source": [
    "- This idea of learning factors of the change matrix relies on the core assumption that weight matrices within a large language model have a lot of linear dependence, as a result of having significantly more parameters than is theoretically required.\n",
    "\n",
    "- The idea behind LoRA is that, once you’ve learned the general task with pre-training, you can do fine tuning with significantly less information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ba12a",
   "metadata": {},
   "source": [
    "From the LoRA paper:\n",
    "\n",
    "> learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed Low-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers’ change during adaptation instead, while keeping the pre-trained weights frozen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f4441",
   "metadata": {},
   "source": [
    "This results in a significantly smaller amount of parameters being trained which means an overall faster and more storage and memory efficient fine tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954eda04",
   "metadata": {},
   "source": [
    "#### Fine-Tuning Flow with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803de1fa",
   "metadata": {},
   "source": [
    "-  first, we freeze the model parameters. We’ll be using these parameters to make inferences, but we won’t update them.\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ae8b590-41ff-42d0-99d2-c481a386ee82_800x173.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425860c9",
   "metadata": {},
   "source": [
    "- We create two matrices. These are sized in such a way that, when they’re multiplied together, they’ll be the same size as the weight matrices of the model we’re fine tuning. \n",
    "- In a large model, with multiple weight matrices, you would create one of these pairs for each weight matrix.\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3dafddc-df1c-4a5d-9a5c-fe5862bc3a83_800x184.png)\n",
    "\n",
    "We then calculate the change matrix:\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd43a7cc8-7610-4aa3-a7a8-3bbd4e2403d1_800x189.png)\n",
    "\n",
    "Then we pass our input through both the frozen weights and the change matrix.\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98a46d28-3340-4d78-9de1-fd2d5d980190_800x384.png)\n",
    "\n",
    "We calculate a loss based on the combination of both outputs then we update matrix A and B based on the loss\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef177f86-d5a3-42f8-a097-c09e3acf7fb9_800x374.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b076c7c9",
   "metadata": {},
   "source": [
    "- We do this operation until we’ve optimized the factors of the change matrix for our fine tuning task. \n",
    "- The backpropagation step to update the matrices A and B is much faster than the process to update the full set of model parameters, on account of A and B being significantly smaller.\n",
    "- This is why, despite more operations in the training process, LoRA is still typically faster than traditional fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf55fc2",
   "metadata": {},
   "source": [
    "#### At Inference\n",
    "\n",
    "When we ultimately want to make inferences with this fine tuned model, we can simply compute the change matrix, and add the changes to the weights. \n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff563e214-005a-4f41-aede-d9c7e3a59a44_800x157.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5547adc",
   "metadata": {},
   "source": [
    "We can also multiply the change matrix by a scaling factor, allowing us to control the level of impact that change matrix has on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc7b6f",
   "metadata": {},
   "source": [
    "#### A Note on LoRA Rank\n",
    "\n",
    "LoRA has a hyperparameter, named r, which describes the depth of the A and B matrix used to construct the change matrix discussed previously. Higher r values mean larger A and B matrices, which means they can encode more linearly independent information in the change matrix.\n",
    "\n",
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2895306a-33a3-402f-94f6-8ed0f36e7589_800x222.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d120ab9",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8def686-26de-40d7-b078-da2a59da124b_800x429.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07bc080",
   "metadata": {},
   "source": [
    "![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd33d2e1c-78db-4749-9146-91b1d3c27d13_800x264.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d661b",
   "metadata": {},
   "source": [
    "Code to checkout: https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/LoRA.ipynb"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

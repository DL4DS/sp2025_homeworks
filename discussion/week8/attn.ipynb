{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What’s the big deal about attention?\n",
    "\n",
    "Consider machine translation as an example. Before attention, most translation was done via an encoder-decoder network. The encoder encodes the input sentence (“I love you”) via a recurrent model and the decoder decodes it into another language (“我爱你”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://eugeneyan.com/assets/encoder-decoder.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Via this approach, the encoder had to cram the entire input into a fixed-size vector which is then passed to the decoder\n",
    "    - this single vector had to convey everything about the input sentence! Naturally, this led to an informational bottleneck. \n",
    "- With attention, we no longer have to encode input sentences into a single vector. \n",
    "    - Instead, we let the decoder attend to different words in the input sentence at each step of output generation. \n",
    "    - This increases the informational capacity, from a single fixed-size vector to the entire sentence (of vectors).\n",
    "\n",
    "- Furthermore, previous recurrent models had long paths between input and output words. \n",
    "    - If you had a 50-word sentence, the decoder had to recall information from 50 steps ago for the first word (and that data had to be squeezed into a single context vector)\n",
    "    - As a result, recurrent models had difficulty dealing with long-range dependencies\n",
    "    - Attention addressed this by letting each step of the decoder see the entire input sentence and decide what words to attend to. \n",
    "\n",
    "- Prior models using the above architecture had to process words sequentially. \n",
    "    - To encode a sentence, we start with the first word (w1) and process it to get the first hidden state (h1). Then, we input the second word (w2) with the previous hidden state (h1) to derive the next hidden state (h2). And so on. \n",
    "    - This was slow and inefficient. \n",
    "    - Attention allowed for parallel processing of words, which sped up training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Embeddings\n",
    "\n",
    "- Embeddings represent words/tokens as high dimensional vectors in a way that similar words are located closed to each other.\n",
    "- Words having high similarity are placed closer to each other whereas words with low similarity will be pushed far away from each other in this space.\n",
    "\n",
    "### How are the words placed together or far apart?\n",
    "\n",
    "- Similar words are placed together by the `context` --  meaning the words in the sentence decide how much closer two words will be placed to each other.\n",
    "- Caluclated by Word Similarity Score (Cosine Similarity, Scaled dot product (used in self-attention))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://lakshyamalhotra.github.io/images/word_similarity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take another example, consider the following two statements:\n",
    "\n",
    "1. A bat lives in the cave\n",
    "2. A tennis racket and a baseball bat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any context, it would be hard for the model to place the word “bat” in the embeddings space – it would probably be placed in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://lakshyamalhotra.github.io/images/attention_weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|         | Bat | Cave | Racket |\n",
    "|---------|-----|------|--------|\n",
    "| **Bat**    | 1   | 0.71 | 0.71   |\n",
    "| **Cave**   | 0.71 | 1   | 0      |\n",
    "| **Racket** | 0.71 | 0   | 1      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above similarity table, we can see the individual contributions (as weights) from other words for a given word. In the language of linear algebra, we can write the individual words as a linear combination (weighted sum) of the other:\n",
    "\n",
    "- A bat lives in the cave\n",
    "    - Bat = 1 * Bat + 0.71 * Cave\n",
    "    - Cave = 0.71 * Bat + 1 * Cave\n",
    "\n",
    "- A tennis racket and a baseball bat\n",
    "    - Bat = 1 * Bat + 0.71 * Racket\n",
    "    - Racket = 0.71 * Bat + 1 * Racket\n",
    "\n",
    "In machine learning applications, it’s convenient to normalize the coefficients so that they sum to 1.\n",
    "    - Normalization increases the interpretability of the coefficients as they can be assumed as the relative weights or the probabilities. This is typically done by applying softmax operation.\n",
    "\n",
    "After softmax:\n",
    "    - Bat = 0.57 * Bat + 0.43 * Cave\n",
    "    - Cave = 0.43 * Bat + 0.57 * Cave\n",
    "\n",
    "    - Bat = 0.57 * Bat + 0.43 * Racket\n",
    "    - Racket = 0.43 * Bat + 0.57 * Racket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what are we doing here?\n",
    "\n",
    "- With the help of some coordinate geometry, it is not hard to see that after adding contributions from the other words, we are basically moving each word in the embedding space.\n",
    "\n",
    "In other words, after getting the “context” from other words in the first sentence, the first set of equations basically shifted the “Bat” and the “Cave” towards each other. The equations,\n",
    "\n",
    "    - Bat = 0.57 * Bat + 0.43 * Cave\n",
    "    - Cave = 0.43 * Bat + 0.57 * Cave\n",
    "\n",
    "are the context vectors for the “Bat” and the “Cave” tokens in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On a very high level this is how the attention mechanism works -- we took a word token (query) and look in its own sequence (keys) to find the information that should be used from other words to create a context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To summarize, we took the following steps to calculate the context vector:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate attention scores: The attention mechanism calculates the similarity scores between each pair of the input sequence. Higher the similarity score, the more relevant is the key to the current query. (cosine similarity used here, but in practice -- scaled dot-product is used for efficiency and stability)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# define coordinates of words\n",
    "racket = [0, 5]  # x-coordinate: 0, y-coordinate: 5\n",
    "bat = [3, 3]     # x-coordinate: 3, y-coordinate: 3\n",
    "cave = [4, 0]    # x-coordinate: 4, y-coordinate: 0\n",
    "\n",
    "# convert the list of coordinates to a NumPy array\n",
    "embed_vectors = np.array([bat, cave, racket])\n",
    "\n",
    "# create a function for creating cosine similarity\n",
    "def cos_sim_matrix(vectors):\n",
    "    # calculate the norms of each vector\n",
    "    norms = norm(vectors, axis=1)\n",
    "\n",
    "    # calculate the dot product between each pair of vectors\n",
    "    dot_products = np.dot(vectors, vectors.T)\n",
    "\n",
    "    # calculate the outer product of the norms\n",
    "    norm_products = np.outer(norms, norms)\n",
    "\n",
    "    # calculate the cosine similarity matrix\n",
    "    cosine_similarity = dot_products / norm_products\n",
    "\n",
    "    return np.round(cosine_similarity, 2)\n",
    "\n",
    "# find similarity\n",
    "similarity_matrix = cos_sim_matrix(embed_vectors)\n",
    "print(similarity_matrix)\n",
    "\n",
    "# prints\n",
    "# array([[1. , 0.71, 0.71],\n",
    "#\t     [0.71, 1. , 0. ],\n",
    "#\t     [0.71, 0. , 1. ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Normalization (Softmax): Softmax function is applied on the attention scores to yield the probabilities. The softmax ensures the weights sum up to 1, which is helpful for training stability and interpretability.\n",
    "\n",
    "```python\n",
    "# similarity scores for the first sentence\n",
    "bat_cave = similarity_matrix[:1, :2]\n",
    "\n",
    "# apply softmax to it\n",
    "bat_cave = np.exp(bat_cave)\n",
    "softmax = bat_cave / np.sum(bat_cave, axis=1)\n",
    "softmax = np.round(softmax, 2)\n",
    "print(softmax)\n",
    "\n",
    "# prints\n",
    "# array([[0.57, 0.43]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Weighted Summation: Lastly, attention weights are multiplied by the corresponding values and these weighted contributions are then summed up to create a context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are query, key, and value vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A “query” is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
    "- The “key” is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match with the query.\n",
    "- The “value” in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values.\n",
    "\n",
    "- These vectors are actually obtained by transforming the input embedding vectors with three matrices – K, Q and V.\n",
    "\n",
    "*\tQ (Query) — “What am I looking for?”\n",
    "*\tK (Key) — “What do I have to offer?”\n",
    "*\tV (Value) — “What information should I share if chosen?”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition behind K, Q and V matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From basic linear-algebra, we know that matrices are nothing but the linear transformations or rules that operate on vectors and change their properties like rotate them by a certain angle, reflect them about some axis, etc\n",
    "- These `trainable matrices` for query, keys and values do something similar – stretch, shear, or elongate the manifolds such that the `similarity of the alike words increases whereas for dissimilar words it decreases.`\n",
    "\n",
    "- In a nutshell, transforming vectors with matrices can increase/decrease the similarity score and hence the attention weights between two vectors.\n",
    "    - This is what K, Q and V do to the input embedding vectors. They are trainable meaning during the course of training, their weights will be optimized to change the manifold. This will increase/decrease the similarity between tokens on the basis of the loss function optimization during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and Attention Mechanism\n",
    "\n",
    "![img](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kf871smtQKXAf3dSYOlVPA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://storrs.io/content/images/2021/08/Screen-Shot-2021-08-07-at-7.51.37-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7sy0KlRgyh3n0M0SH-6xqQ.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Submit a \"Query\" for each token in the input sequence\n",
    "### 2. Take those queries and match them against the \"Keys\" -- The Query is essentially probing the Keys to see how relevant they are to the current token.\n",
    "### 3. The similarity between the Query and the Keys is computed using a dot product -- the attention matrix is then formed by taking the softmax of the dot products.\n",
    "### 4. The attention matrix is a representation of how much each query matches a given key - the attention values indicate how much each token in the sequence is \"paying attention\" to other tokens in the sequence.\n",
    "### 5. The attention values are then used to weight the values associated with each key, producing a weighted sum that represents the output for each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention - Q, K, V's all from the same sequence\n",
    "### Cross Attention - Q from one sequence and K, V from another sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://storrs.io/content/images/size/w1600/2021/08/image3--8-.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://storrs.io/content/images/size/w1600/2021/08/image8--1-.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storrs.io/content/images/size/w1600/2021/08/image7--2-.png\" alt=\"Description of Image\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storrs.io/content/images/size/w1600/2021/08/image4--2-.png\" alt=\"Description of Image\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storrs.io/content/images/size/w1600/2021/08/image6--3-.png\" alt=\"Description of Image\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storrs.io/content/images/size/w1600/2021/08/image5--3-.png\" alt=\"Alt Text\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YE0dWWP7uzWIa5zmNN2xdA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Helper class for Self-Attention calculation\n",
    "class SelfAttention(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.output_dim = output_dim\n",
    "\n",
    "\t\t# matrices for Query, Key, and Value\n",
    "\t\tself.W_query = nn.Linear(\n",
    "\t\t\tin_features=input_dim, out_features=output_dim, bias=False\n",
    "\t\t)\n",
    "\t\tself.W_key = nn.Linear(\n",
    "\t\t\tin_features=input_dim, out_features=output_dim, bias=False\n",
    "\t\t)\n",
    "\t\tself.W_value = nn.Linear(\n",
    "\t\t\tin_features=input_dim, out_features=output_dim, bias=False\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x): # x shape: (N, input_dim), N: number of tokens\n",
    "\t\tqueries = self.W_query(x)\n",
    "\t\tkeys = self.W_key(x)\n",
    "\t\tvalues = self.W_value(x)\n",
    "\n",
    "\t\t# attention scores\n",
    "\t\tattn_scores = queries @ keys.T # N x N\n",
    "\n",
    "\t\t# attention weights\n",
    "\t\tattn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "\n",
    "\t\t# compute context vector\n",
    "\t\tcontext_vec = attn_weights @ values # N x output_dim\n",
    "\n",
    "\t\treturn context_vec\n",
    "\n",
    "# Multi-head self-attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, num_heads):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.heads = [\n",
    "\t\t\tSelfAttention(input_dim, output_dim)\n",
    "\t\t\tfor _ in range(num_heads)\n",
    "\t\t]\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./pos_emb.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
